{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMJAa3ik1oRxDtndc2DW/vk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ashail33/Paper/blob/master/One%20script.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Environment prep**"
      ],
      "metadata": {
        "id": "ImzJDJofLgxV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Set up for data generation\n",
        "!pip install git+https://github.com/Ashail33/mdcgenpy.git\n",
        "\n",
        "from  mdcgenpy import clusters as cl\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "import glob\n",
        "import re\n",
        "from google.oauth2 import service_account\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.errors import HttpError\n",
        "from googleapiclient.http import MediaFileUpload\n",
        "# import cl\n",
        "from google.colab import drive\n",
        "\n",
        "# Set up for model creation and evaluation\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.2.0/spark-3.2.0-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.0-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "import os\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.0-bin-hadoop3.2\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"my-app-name\").master(\"local[*]\").getOrCreate()\n",
        "\n",
        "\n",
        "\n",
        "from pyspark.ml.clustering import KMeans, KMeansModel\n",
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.sql import SparkSession\n",
        "from sklearn.base import BaseEstimator, ClusterMixin\n",
        "import numpy as np\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml.clustering import KMeans as PKMeans\n",
        "from sklearn.cluster import KMeans as SKKMeans\n",
        "from pyspark.ml.evaluation import ClusteringEvaluator\n",
        "import numpy as np\n",
        "\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.metrics import davies_bouldin_score, silhouette_score, calinski_harabasz_score, adjusted_rand_score, normalized_mutual_info_score\n",
        "from scipy.spatial.distance import cdist\n",
        "import time\n",
        "import psutil\n",
        "import os\n",
        "import tracemalloc\n",
        "from google.colab import drive\n",
        "import glob\n",
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans, DBSCAN, SpectralClustering\n",
        "from sklearn.mixture import GaussianMixture\n",
        "import json\n",
        "\n",
        "from sklearn.neighbors import KDTree\n",
        "import numpy as np\n",
        "from sklearn.metrics import pairwise_distances"
      ],
      "metadata": {
        "id": "tIGNr_dhLhW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Create Clustering Dataset**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-BuB0D1XLSIH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The properties I will be adding to the dataset are:\n",
        "#     1) Outliers - Binary ( two options) - outliers\n",
        "#     2) Noise - Binary ( two options) add_noise=0,n_noise=None,\n",
        "#     3) Number of clusters - I will select three values k\n",
        "#     4) Number of data points - I will select three values ( 100 000, 1 000 000, 100 000 000)n_samples\n",
        "#     5) Number of features - I will select five values ( 2, 10, 50, 100, 500) n_feats\n",
        "#     6) Density - I will select three values compactness_factor\n",
        "#     --7) Cluster shape - I will select three types - clusters within a radius ,  hollow shaped clusters , s-shaped / c-shaped clusters\n",
        "#      distributions\n",
        "#         'uniform': lambda shape, param: np.random.uniform(-param, param, shape),\n",
        "#     'gaussian': lambda shape, param: np.random.normal(0, param, shape),\n",
        "#     'logistic': lambda shape, param: np.random.logistic(0, param, shape),\n",
        "#     'triangular': lambda shape, param: np.random.triangular(-param, 0, param, shape),\n",
        "#     'gamma': lambda shape, param: np.random.gamma(2 + 8 * np.random.rand(), param / 5, shape),\n",
        "#     'gap': lambda shape, param: gap(shape, param)\n",
        "\n",
        "# --8) Missing values - this will need to be created by randomly removing values up to a certain number of columns and records\n",
        "\n",
        "#Mount drive where you would like to add the data to\n",
        "\n",
        "drive.mount('/content/gdrive', force_remount=False)\n",
        "\n",
        "# Set the path to the 'Masters_data' folder in your Google Drive\n",
        "base_path = '/content/gdrive/MyDrive/'\n",
        "folder_name = 'Masters_data'\n",
        "folder_path = os.path.join(base_path, folder_name)\n",
        "\n",
        "# Create the 'Masters_data' folder if it doesn't exist\n",
        "if not os.path.exists(folder_path):\n",
        "    os.makedirs(folder_path)\n",
        "\n",
        "# Parameters for generating datasets\n",
        "outliers = [0,500]\n",
        "noise_options = [(0, None), (10, 100)]\n",
        "n_clusters = [3, 5, 10]\n",
        "n_samples = [10000,50000, 100000]\n",
        "n_feats = [2, 10,100]\n",
        "compactness_factors = [0.01, 0.5, 1]\n",
        "\n",
        "# Distributions for cluster shapes\n",
        "dist_options = ['gaussian']\n",
        "\n",
        "# Find the last generated dataset\n",
        "existing_files = glob.glob(os.path.join(folder_path, 'dataset_*.csv'))\n",
        "dataset_numbers = [int(re.search(r'dataset_(\\d+).csv', file).group(1)) for file in existing_files]\n",
        "if dataset_numbers:\n",
        "    last_dataset_id = max(dataset_numbers)\n",
        "else:\n",
        "    last_dataset_id = -1\n",
        "\n",
        "metadata = []\n",
        "\n",
        "# Continue from the last generated dataset\n",
        "for idx, outlier in enumerate(outliers):\n",
        "    for idy, (add_noise, n_noise) in enumerate(noise_options):\n",
        "        for idz, k in enumerate(n_clusters):\n",
        "            for idw, n_sample in enumerate(n_samples):\n",
        "                for idv, n_feat in enumerate(n_feats):\n",
        "                    for idu, compactness_factor in enumerate(compactness_factors):\n",
        "                        for idt, distribution in enumerate(dist_options):\n",
        "                            current_dataset_id = (idx * len(noise_options) * len(n_clusters) * len(n_samples) * len(n_feats) * len(compactness_factors) * len(dist_options) +\n",
        "                                                  idy * len(n_clusters) * len(n_samples) * len(n_feats) * len(compactness_factors) * len(dist_options) +\n",
        "                                                  idz * len(n_samples) * len(n_feats) * len(compactness_factors) * len(dist_options) +\n",
        "                                                  idw * len(n_feats) * len(compactness_factors) * len(dist_options) +\n",
        "                                                  idv * len(compactness_factors) * len(dist_options) +\n",
        "                                                  idu * len(dist_options) +\n",
        "                                                  idt)\n",
        "\n",
        "                            if current_dataset_id <= last_dataset_id:\n",
        "                                continue\n",
        "\n",
        "                            file_name = f'dataset_{current_dataset_id}.csv'\n",
        "                            file_path = os.path.join(folder_path, file_name)\n",
        "                            distributions = [distribution] * k\n",
        "\n",
        "\n",
        "                            cluster_gen = cl.ClusterGenerator(\n",
        "                                          n_samples=n_sample,\n",
        "                                          outliers=outlier,\n",
        "                                          n_feats=n_feat,\n",
        "                                          k=k,\n",
        "                                          distributions=distributions,\n",
        "                                          compactness_factor=compactness_factor,\n",
        "\n",
        "                                          )\n",
        "\n",
        "                            data = cluster_gen.generate_data(output_file=file_path)\n",
        "\n",
        "                            dataset_properties = {\n",
        "                                'id': current_dataset_id,\n",
        "                                'outliers': outlier,\n",
        "                                'add_noise': add_noise,\n",
        "                                'n_noise': n_noise,'n_clusters': k,\n",
        "                                'n_samples': n_sample,\n",
        "                                'n_feats': n_feat,\n",
        "                                'compactness_factor': compactness_factor,\n",
        "                                'distribution': distribution,\n",
        "                                'file_path': file_path\n",
        "                                }\n",
        "\n",
        "                            metadata.append(dataset_properties)\n",
        "                            metadata_file_path = os.path.join(folder_path, 'metadata.json')\n",
        "                            with open(metadata_file_path, 'w') as f:\n",
        "                                json.dump(metadata, f)\n"
      ],
      "metadata": {
        "id": "dvz7jsKwLe3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Create the models**"
      ],
      "metadata": {
        "id": "N0yx-q0VPFvg"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WdbsvszsPMqc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}