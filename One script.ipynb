{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNKM2avp+dVDh/F94aXJ2kF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ashail33/Paper/blob/master/One%20script.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1) Environment prep**"
      ],
      "metadata": {
        "id": "ImzJDJofLgxV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Set up for data generation\n",
        "!pip install git+https://github.com/Ashail33/mdcgenpy.git\n",
        "\n",
        "from  mdcgenpy import clusters as cl\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "import glob\n",
        "import re\n",
        "from google.oauth2 import service_account\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.errors import HttpError\n",
        "from googleapiclient.http import MediaFileUpload\n",
        "# import cl\n",
        "from google.colab import drive\n",
        "\n",
        "# Set up for model creation and evaluation\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.2.0/spark-3.2.0-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.0-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "import os\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.0-bin-hadoop3.2\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"my-app-name\").master(\"local[*]\").getOrCreate()\n",
        "\n",
        "\n",
        "\n",
        "from pyspark.ml.clustering import KMeans, KMeansModel\n",
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.sql import SparkSession\n",
        "from sklearn.base import BaseEstimator, ClusterMixin\n",
        "import numpy as np\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml.clustering import KMeans as PKMeans\n",
        "from sklearn.cluster import KMeans as SKKMeans\n",
        "from pyspark.ml.evaluation import ClusteringEvaluator\n",
        "import numpy as np\n",
        "\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.metrics import davies_bouldin_score, silhouette_score, calinski_harabasz_score, adjusted_rand_score, normalized_mutual_info_score\n",
        "from scipy.spatial.distance import cdist\n",
        "import time\n",
        "import psutil\n",
        "import os\n",
        "import tracemalloc\n",
        "from google.colab import drive\n",
        "import glob\n",
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans, DBSCAN, SpectralClustering\n",
        "from sklearn.mixture import GaussianMixture\n",
        "import json\n",
        "\n",
        "from sklearn.neighbors import KDTree\n",
        "import numpy as np\n",
        "from sklearn.metrics import pairwise_distances"
      ],
      "metadata": {
        "id": "tIGNr_dhLhW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2) Create Clustering Dataset**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-BuB0D1XLSIH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The properties I will be adding to the dataset are:\n",
        "#     1) Outliers - Binary ( two options) - outliers\n",
        "#     2) Noise - Binary ( two options) add_noise=0,n_noise=None,\n",
        "#     3) Number of clusters - I will select three values k\n",
        "#     4) Number of data points - I will select three values ( 100 000, 1 000 000, 100 000 000)n_samples\n",
        "#     5) Number of features - I will select five values ( 2, 10, 50, 100, 500) n_feats\n",
        "#     6) Density - I will select three values compactness_factor\n",
        "#     --7) Cluster shape - I will select three types - clusters within a radius ,  hollow shaped clusters , s-shaped / c-shaped clusters\n",
        "#      distributions\n",
        "#         'uniform': lambda shape, param: np.random.uniform(-param, param, shape),\n",
        "#     'gaussian': lambda shape, param: np.random.normal(0, param, shape),\n",
        "#     'logistic': lambda shape, param: np.random.logistic(0, param, shape),\n",
        "#     'triangular': lambda shape, param: np.random.triangular(-param, 0, param, shape),\n",
        "#     'gamma': lambda shape, param: np.random.gamma(2 + 8 * np.random.rand(), param / 5, shape),\n",
        "#     'gap': lambda shape, param: gap(shape, param)\n",
        "\n",
        "# --8) Missing values - this will need to be created by randomly removing values up to a certain number of columns and records\n",
        "\n",
        "#Mount drive where you would like to add the data to\n",
        "\n",
        "drive.mount('/content/gdrive', force_remount=False)\n",
        "\n",
        "# Set the path to the 'Masters_data' folder in your Google Drive\n",
        "base_path = '/content/gdrive/MyDrive/'\n",
        "folder_name = 'Masters_data'\n",
        "folder_path = os.path.join(base_path, folder_name)\n",
        "\n",
        "# Create the 'Masters_data' folder if it doesn't exist\n",
        "if not os.path.exists(folder_path):\n",
        "    os.makedirs(folder_path)\n",
        "\n",
        "# Parameters for generating datasets\n",
        "outliers = [0,500]\n",
        "noise_options = [(0, None), (10, 100)]\n",
        "n_clusters = [3, 5, 10]\n",
        "n_samples = [10000,50000, 100000]\n",
        "n_feats = [2, 10,100]\n",
        "compactness_factors = [0.01, 0.5, 1]\n",
        "\n",
        "# Distributions for cluster shapes\n",
        "dist_options = ['gaussian']\n",
        "\n",
        "# Find the last generated dataset\n",
        "existing_files = glob.glob(os.path.join(folder_path, 'dataset_*.csv'))\n",
        "dataset_numbers = [int(re.search(r'dataset_(\\d+).csv', file).group(1)) for file in existing_files]\n",
        "if dataset_numbers:\n",
        "    last_dataset_id = max(dataset_numbers)\n",
        "else:\n",
        "    last_dataset_id = -1\n",
        "\n",
        "metadata = []\n",
        "\n",
        "# Continue from the last generated dataset\n",
        "for idx, outlier in enumerate(outliers):\n",
        "    for idy, (add_noise, n_noise) in enumerate(noise_options):\n",
        "        for idz, k in enumerate(n_clusters):\n",
        "            for idw, n_sample in enumerate(n_samples):\n",
        "                for idv, n_feat in enumerate(n_feats):\n",
        "                    for idu, compactness_factor in enumerate(compactness_factors):\n",
        "                        for idt, distribution in enumerate(dist_options):\n",
        "                            current_dataset_id = (idx * len(noise_options) * len(n_clusters) * len(n_samples) * len(n_feats) * len(compactness_factors) * len(dist_options) +\n",
        "                                                  idy * len(n_clusters) * len(n_samples) * len(n_feats) * len(compactness_factors) * len(dist_options) +\n",
        "                                                  idz * len(n_samples) * len(n_feats) * len(compactness_factors) * len(dist_options) +\n",
        "                                                  idw * len(n_feats) * len(compactness_factors) * len(dist_options) +\n",
        "                                                  idv * len(compactness_factors) * len(dist_options) +\n",
        "                                                  idu * len(dist_options) +\n",
        "                                                  idt)\n",
        "\n",
        "                            if current_dataset_id <= last_dataset_id:\n",
        "                                continue\n",
        "\n",
        "                            file_name = f'dataset_{current_dataset_id}.csv'\n",
        "                            file_path = os.path.join(folder_path, file_name)\n",
        "                            distributions = [distribution] * k\n",
        "\n",
        "\n",
        "                            cluster_gen = cl.ClusterGenerator(\n",
        "                                          n_samples=n_sample,\n",
        "                                          outliers=outlier,\n",
        "                                          n_feats=n_feat,\n",
        "                                          k=k,\n",
        "                                          distributions=distributions,\n",
        "                                          compactness_factor=compactness_factor,\n",
        "\n",
        "                                          )\n",
        "\n",
        "                            data = cluster_gen.generate_data(output_file=file_path)\n",
        "\n",
        "                            dataset_properties = {\n",
        "                                'id': current_dataset_id,\n",
        "                                'outliers': outlier,\n",
        "                                'add_noise': add_noise,\n",
        "                                'n_noise': n_noise,'n_clusters': k,\n",
        "                                'n_samples': n_sample,\n",
        "                                'n_feats': n_feat,\n",
        "                                'compactness_factor': compactness_factor,\n",
        "                                'distribution': distribution,\n",
        "                                'file_path': file_path\n",
        "                                }\n",
        "\n",
        "                            metadata.append(dataset_properties)\n",
        "                            metadata_file_path = os.path.join(folder_path, 'metadata.json')\n",
        "                            with open(metadata_file_path, 'w') as f:\n",
        "                                json.dump(metadata, f)\n"
      ],
      "metadata": {
        "id": "dvz7jsKwLe3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3) Create the model functions**"
      ],
      "metadata": {
        "id": "N0yx-q0VPFvg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1) Standard SK-learn models\n",
        "\n"
      ],
      "metadata": {
        "id": "iYeBvFV-Xn0m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import MiniBatchKMeans\n"
      ],
      "metadata": {
        "id": "WdbsvszsPMqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2) Map reduce k-means"
      ],
      "metadata": {
        "id": "9jTzmFyqX16a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.clustering import KMeans as PKMeans\n",
        "from pyspark.ml.evaluation import ClusteringEvaluator\n",
        "from sklearn.cluster import KMeans as SKKMeans\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.linalg import Vectors\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris, load_digits, make_blobs\n",
        "from sklearn.cluster import KMeans as SKKMeans\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.linalg import Vectors\n",
        "import matplotlib.pyplot as plt\n",
        "import timeit\n",
        "from pyspark.ml.evaluation import ClusteringEvaluator\n",
        "# Set up SparkSession\n",
        "spark = SparkSession.builder.appName(\"MapReduceKMeans\").getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "from pyspark.ml.clustering import KMeans as PKMeans\n",
        "\n",
        "class MapReduceKMeans:\n",
        "    def __init__(self, n_clusters, batch_size, max_iterations):\n",
        "        self.n_clusters = n_clusters\n",
        "        self.batch_size = batch_size\n",
        "        self.max_iterations = max_iterations\n",
        "\n",
        "    def fit_predict(self, data):\n",
        "        \"\"\"\n",
        "        MapReduce KMeans implementation using PySpark.\n",
        "        \"\"\"\n",
        "        spark = SparkSession.builder.getOrCreate()\n",
        "        sc = spark.sparkContext\n",
        "        sc.setLogLevel(\"ERROR\")\n",
        "        # Convert numpy array to PySpark DataFrame\n",
        "        df = sc.parallelize(data.tolist()).map(lambda x: (Vectors.dense(x), )).toDF([\"features\"])\n",
        "        # Initialize centroids using KMeans++ initialization\n",
        "        initial_centroids = PKMeans().fit(df.select(\"features\")).clusterCenters()\n",
        "        global_centroids = initial_centroids\n",
        "        for i in range(0, data.shape[0], self.batch_size):\n",
        "            # Split data into batches\n",
        "            local_data = data[i:i+self.batch_size]\n",
        "            local_df = sc.parallelize(local_data.tolist()).map(lambda x: (Vectors.dense(x), )).toDF([\"features\"])\n",
        "            # Train local KMeans model\n",
        "            local_kmeans = PKMeans(k=self.n_clusters, initMode=\"k-means||\", maxIter=self.max_iterations, seed=42)\n",
        "            local_model = local_kmeans.fit(local_df.select(\"features\"))\n",
        "            local_centroids = local_model.clusterCenters()\n",
        "            # Update global centroids\n",
        "            global_centroids = np.vstack([global_centroids, local_centroids])\n",
        "            global_centroids = SKKMeans(n_clusters=self.n_clusters, random_state=42).fit(global_centroids).cluster_centers_\n",
        "        # Final KMeans model with global centroids\n",
        "        kmeans = PKMeans(k=self.n_clusters, initMode=\"k-means||\", maxIter=self.max_iterations, seed=42)\n",
        "        final_model = kmeans.fit(df.select(\"features\"))\n",
        "        evaluator = ClusteringEvaluator()\n",
        "        global_cost = evaluator.evaluate(final_model.transform(df))\n",
        "        global_centroids = final_model.clusterCenters()\n",
        "        return KMeans(n_clusters=self.n_clusters, init=global_centroids, n_init=1, random_state=42).fit_predict(data)"
      ],
      "metadata": {
        "id": "Nqv2VyFYYYD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consensus Clustering"
      ],
      "metadata": {
        "id": "1LhVk9vVZshw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#use this function for weighted consensus method, it has it's issues like the fit to complex shapes irrespective of the underlying model, the spectral clustering of the affinity matrix makes this tricky. This is definitely a shortfall of the method\n",
        "\n",
        "\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.cluster import SpectralClustering, KMeans\n",
        "import numpy as np\n",
        "\n",
        "class ConsensusClustering:\n",
        "    def __init__(self, n_clusters, model_funcs, weights=None, sampling_rate=0.8, random_state=None):\n",
        "        self.n_clusters = n_clusters\n",
        "        self.model_funcs = model_funcs\n",
        "        self.weights = weights\n",
        "        self.sampling_rate = sampling_rate\n",
        "        self.random_state = random_state\n",
        "        self.co_association_matrix = None\n",
        "\n",
        "    def fit_predict(self, data):\n",
        "        n_samples = data.shape[0]\n",
        "        n_models = len(self.model_funcs)\n",
        "\n",
        "        if self.weights is None:\n",
        "            self.weights = np.ones(n_models)\n",
        "\n",
        "        if len(self.weights) != n_models:\n",
        "            raise ValueError(\"The number of weights must be equal to the number of models.\")\n",
        "\n",
        "        self.weights = self.weights / np.sum(self.weights)\n",
        "\n",
        "        self.co_association_matrix = np.zeros((n_samples, n_samples))\n",
        "\n",
        "        for i in range(n_models):\n",
        "            model_func = self.model_funcs[i]\n",
        "            model = model_func(self.n_clusters)\n",
        "            sampled_indices = np.random.choice(n_samples, int(n_samples * self.sampling_rate), replace=True)\n",
        "            sampled_data = data[sampled_indices]\n",
        "\n",
        "            if callable(model):\n",
        "                labels = model(sampled_data)\n",
        "            else:\n",
        "                labels = model.fit_predict(sampled_data)\n",
        "\n",
        "            for label in range(self.n_clusters):\n",
        "                cluster_indices = np.where(labels == label)[0]\n",
        "                original_indices = sampled_indices[cluster_indices]\n",
        "                pairs = itertools.combinations(original_indices, 2)\n",
        "                for x, y in pairs:\n",
        "                    self.co_association_matrix[x, y] += self.weights[i]\n",
        "                    self.co_association_matrix[y, x] += self.weights[i]\n",
        "\n",
        "        spectral_clustering = SpectralClustering(n_clusters=self.n_clusters, affinity='precomputed')\n",
        "        final_labels = spectral_clustering.fit_predict(self.co_association_matrix)\n",
        "\n",
        "        return final_labels\n",
        "\n",
        "\n",
        "# Model functions for k-means and Agglomerative clustering\n",
        "def kmeans_model_func(n_clusters, random_state):\n",
        "    return KMeans(n_clusters=n_clusters, random_state=random_state)\n",
        "\n",
        "def agglomerative_model_func(n_clusters, random_state):\n",
        "    return AgglomerativeClustering(n_clusters=n_clusters)\n",
        "\n",
        "def dbscan_model_func(n_clusters):\n",
        "    return DBSCAN(eps=0.15, min_samples=10)\n",
        "\n",
        "def optics_model_func(n_clusters, random_state):\n",
        "    return OPTICS(min_samples=1, xi=0.5, cluster_method='xi', n_jobs=-1)\n",
        "\n",
        "def optics_DB_Scan_model_func(n_clusters, random_state):\n",
        "    def optics_instance(data):\n",
        "        optics = OPTICS(min_samples=1, n_jobs=-1)\n",
        "        optics.fit(data)\n",
        "        eps = np.partition(optics.reachability_[optics.ordering_], n_clusters - 1)[n_clusters - 1]\n",
        "        labels = cluster_optics_dbscan(reachability=optics.reachability_,\n",
        "                                       core_distances=optics.core_distances_,\n",
        "                                       ordering=optics.ordering_,\n",
        "                                       eps=eps)\n",
        "        return labels\n",
        "\n",
        "    return optics_instance\n",
        "\n",
        "# # Define your models in a list\n",
        "# model_funcs = [KMeans]*5\n",
        "\n",
        "# # Define the weights for the models\n",
        "# weights = [0.5, 0.2,0.003,0.4,3]\n",
        "\n",
        "# # Initialize ConsensusClustering\n",
        "# consensus_clustering = ConsensusClustering(n_clusters=2, model_funcs=model_funcs, weights=weights, sampling_rate=0.8)\n",
        "\n",
        "# # Load your data\n",
        "# data, _ = make_moons(n_samples=200, noise=0.05, random_state=0)\n",
        "\n",
        "# # Apply consensus clustering\n",
        "# labels = consensus_clustering.fit_predict(data)\n",
        "\n",
        "# # `labels` now contains the cluster assignments for each sample in `data`.\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# # Plot the data points with colors corresponding to the assigned cluster\n",
        "# plt.scatter(data[:, 0], data[:, 1], c=labels, cmap='viridis', alpha=0.6)\n",
        "\n",
        "# # Title and axis labels\n",
        "# plt.title('Consensus Clustering Results')\n",
        "# plt.xlabel('Feature 1')\n",
        "# plt.ylabel('Feature 2')\n",
        "\n",
        "# # Show the plot\n",
        "# plt.show()\n"
      ],
      "metadata": {
        "id": "ePVz3h7AZtEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "S5C"
      ],
      "metadata": {
        "id": "ayvJjeU1acoH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cvxpy as cp\n",
        "from scipy.sparse import csr_matrix, diags, eye, issparse\n",
        "from scipy.sparse.linalg import eigsh\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "class S5C:\n",
        "    def __init__(self, n_clusters, sparsity_param=0.1, max_iter=1000, tol=1e-4, batch_size=10, random_state=None):\n",
        "        self.n_clusters = n_clusters\n",
        "        self.sparsity_param = sparsity_param\n",
        "        self.max_iter = max_iter\n",
        "        self.tol = tol\n",
        "        self.batch_size = batch_size\n",
        "        self.random_state = random_state\n",
        "        self.labels_ = None\n",
        "\n",
        "    def _compute_sparse_representation(self, X, target_idx, selected_samples):\n",
        "        \"\"\"Solve Lasso for each data point using CVXPY to get sparse representation.\"\"\"\n",
        "        # Exclude the target index from selected_samples\n",
        "        selected_samples = [idx for idx in selected_samples if idx != target_idx]\n",
        "        if not selected_samples:\n",
        "            return np.zeros(X.shape[0])  # Number of samples\n",
        "\n",
        "        target = X[target_idx, :]  # Shape: (n_features,)\n",
        "\n",
        "        # Selected samples subset\n",
        "        X_subset = X[selected_samples, :]  # Shape: (n_samples_subset, n_features)\n",
        "\n",
        "        # Define the variable\n",
        "        c = cp.Variable(len(selected_samples))\n",
        "\n",
        "        # Define the problem\n",
        "        objective = cp.Minimize(\n",
        "            0.5 * cp.sum_squares(X_subset.T @ c - target) + self.sparsity_param * cp.norm1(c)\n",
        "        )\n",
        "        problem = cp.Problem(objective)\n",
        "\n",
        "        # Solve the problem using a conic solver\n",
        "        # problem.solve(solver='ECOS')\n",
        "        problem.solve(solver=cp.CLARABEL)\n",
        "\n",
        "        # Check if the problem was solved successfully\n",
        "        if problem.status not in [cp.OPTIMAL, cp.OPTIMAL_INACCURATE]:\n",
        "            # If the problem is not solved, return zeros\n",
        "            return np.zeros(X.shape[0])\n",
        "\n",
        "        # Get the coefficients\n",
        "        coef = c.value\n",
        "\n",
        "        # Create full coefficient vector\n",
        "        full_coef = np.zeros(X.shape[0])\n",
        "        full_coef[selected_samples] = coef\n",
        "\n",
        "        return full_coef\n",
        "\n",
        "    def _selective_sampling(self, X):\n",
        "        \"\"\"Selective sampling with incremental addition based on subgradient violations.\"\"\"\n",
        "        n_samples = X.shape[0]\n",
        "        data, rows, cols = [], [], []\n",
        "\n",
        "        # Initialize random generator\n",
        "        rng = np.random.default_rng(self.random_state)\n",
        "\n",
        "        selected_samples = rng.choice(n_samples, self.batch_size, replace=False).tolist()\n",
        "\n",
        "        for i in range(n_samples):\n",
        "            # Compute sparse representation for each data point\n",
        "            coef = self._compute_sparse_representation(X, i, selected_samples)\n",
        "\n",
        "            # Append non-zero coefficients to sparse matrix data\n",
        "            nz_indices = np.nonzero(coef)[0]\n",
        "            coef_nz = coef[nz_indices]\n",
        "\n",
        "            # Use absolute values of coefficients to ensure non-negative affinity matrix\n",
        "            data.extend(np.abs(coef_nz))\n",
        "            rows.extend(nz_indices)\n",
        "            cols.extend([i] * len(nz_indices))\n",
        "\n",
        "            # Calculate subgradient violations\n",
        "            violations = np.abs(coef_nz) - self.sparsity_param\n",
        "            max_violation_idx = np.argmax(violations)\n",
        "            if violations[max_violation_idx] > self.tol:\n",
        "                violating_sample = nz_indices[max_violation_idx]\n",
        "                if violating_sample not in selected_samples:\n",
        "                    selected_samples.append(violating_sample)\n",
        "\n",
        "        # Create a sparse matrix from sampled data\n",
        "        C = csr_matrix((data, (rows, cols)), shape=(n_samples, n_samples))\n",
        "        return C\n",
        "\n",
        "    def _spectral_clustering(self, W):\n",
        "        \"\"\"Spectral clustering with sparse affinity matrix and normalized Laplacian.\"\"\"\n",
        "        # Compute the degree matrix\n",
        "        degrees = np.array(W.sum(axis=1)).flatten()\n",
        "        # Handle zero degrees to avoid division by zero\n",
        "        degrees[degrees <= 1e-12] = 1e-12\n",
        "        D_inv_sqrt = diags(1.0 / np.sqrt(degrees))\n",
        "\n",
        "        # Compute the normalized Laplacian\n",
        "        L = eye(W.shape[0]) - D_inv_sqrt @ W @ D_inv_sqrt\n",
        "\n",
        "        # Ensure L is symmetric\n",
        "        L = (L + L.T) / 2\n",
        "\n",
        "        # Compute the smallest k eigenvectors of the Laplacian\n",
        "        eig_vals, eig_vecs = eigsh(L, k=self.n_clusters, which='SM')\n",
        "        return eig_vecs\n",
        "\n",
        "    def fit(self, X):\n",
        "        \"\"\"Fit the S5C model.\"\"\"\n",
        "        # Ensure X is a NumPy array\n",
        "        if issparse(X):\n",
        "            X = X.toarray()\n",
        "\n",
        "        # Normalize data\n",
        "        X = normalize(X, norm='l2')\n",
        "\n",
        "        C = self._selective_sampling(X)\n",
        "        W = C + C.T  # Symmetrize the affinity matrix\n",
        "\n",
        "        # Ensure W is non-negative\n",
        "        W.data = np.maximum(W.data, 0)\n",
        "\n",
        "        # Perform spectral clustering\n",
        "        U = self._spectral_clustering(W)\n",
        "        U_normalized = normalize(U, norm='l2')\n",
        "\n",
        "        # K-means on spectral embeddings\n",
        "        kmeans = KMeans(n_clusters=self.n_clusters, random_state=self.random_state)\n",
        "        self.labels_ = kmeans.fit_predict(U_normalized)\n",
        "\n",
        "        return self"
      ],
      "metadata": {
        "id": "jV4084N1adm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4) Create the Evaluation function**"
      ],
      "metadata": {
        "id": "Gem7BXXjWv2k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "#evaluation code\n",
        "class Timer:\n",
        "    def __enter__(self):\n",
        "        self.start_time = time.monotonic()\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "        self.interval = time.monotonic() - self.start_time\n",
        "\n",
        "\n",
        "class MemoryMonitor:\n",
        "    def __enter__(self):\n",
        "        self.process = psutil.Process(os.getpid())\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "        self.memory_info = self.process.memory_info()\n",
        "\n",
        "\n",
        "class DiskMonitor:\n",
        "    def __enter__(self):\n",
        "        self.disk_read_start = psutil.disk_io_counters().read_bytes\n",
        "        self.disk_write_start = psutil.disk_io_counters().write_bytes\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "        self.disk_read_end = psutil.disk_io_counters().read_bytes\n",
        "        self.disk_write_end = psutil.disk_io_counters().write_bytes\n",
        "        self.disk_read_bytes = self.disk_read_end - self.disk_read_start\n",
        "        self.disk_write_bytes = self.disk_write_end - self.disk_write_start\n",
        "\n",
        "\n",
        "def compactness(X, labels):\n",
        "    labels = labels.flatten()\n",
        "    unique_labels = np.unique(labels)\n",
        "    centroids = np.array([np.mean(X[labels == i], axis=0) for i in unique_labels])\n",
        "    distances = cdist(X, centroids)\n",
        "    return np.sum(np.min(distances, axis=1))\n",
        "\n",
        "\n",
        "def separation(X, labels):\n",
        "    labels = labels.flatten()\n",
        "    unique_labels = np.unique(labels)\n",
        "    centroids = np.array([np.mean(X[labels == i], axis=0) for i in unique_labels])\n",
        "    return np.sum(cdist(centroids, centroids))\n",
        "\n",
        "\n",
        "\n",
        "def batched_silhouette_score(X, labels, batch_size=100000):\n",
        "    labels = labels.flatten()\n",
        "    n_samples = len(X)\n",
        "    n_batches = int(np.ceil(n_samples / batch_size))\n",
        "\n",
        "    silhouette_sum = 0\n",
        "\n",
        "    for i in range(n_batches):\n",
        "        start = i * batch_size\n",
        "        end = min(start + batch_size, n_samples)\n",
        "\n",
        "        pairwise_distances = metrics.pairwise_distances(X[start:end], X)\n",
        "\n",
        "        a = np.array([np.mean(pairwise_distances[i, labels == labels[start + i]]) for i in range(end-start)])\n",
        "        b = np.array([np.min([np.mean(pairwise_distances[i, labels == cur_label])\n",
        "                              for cur_label in set(labels) - {labels[start + i]}])\n",
        "                      for i in range(end-start)])\n",
        "\n",
        "        silhouette_sum += np.sum((b - a) / np.maximum(a, b))\n",
        "\n",
        "    return silhouette_sum / n_samples\n",
        "\n",
        "def dunn_index(X, labels):\n",
        "    labels = labels.flatten()\n",
        "    unique_labels = np.unique(labels)\n",
        "    clusters = [X[labels == i] for i in unique_labels]\n",
        "    max_intra_cluster_distance = np.max([np.max(cdist(c1, c1)) for c1 in clusters])\n",
        "\n",
        "    min_inter_cluster_distance = np.inf\n",
        "    for i, c1 in enumerate(clusters):\n",
        "        for j, c2 in enumerate(clusters):\n",
        "            if i < j:\n",
        "                min_distance = np.min(cdist(c1, c2))\n",
        "                if min_distance < min_inter_cluster_distance:\n",
        "                    min_inter_cluster_distance = min_distance\n",
        "\n",
        "    return min_inter_cluster_distance / max_intra_cluster_distance\n",
        "\n",
        "from scipy.spatial.distance import pdist, cdist\n",
        "\n",
        "def batched_dunn_index(X, labels, batch_size=100000):\n",
        "    n_samples = len(X)\n",
        "    n_batches = int(np.ceil(n_samples / batch_size))\n",
        "\n",
        "    max_intra_cluster_distance = -np.inf\n",
        "    min_inter_cluster_distance = np.inf\n",
        "\n",
        "    for i in range(n_batches):\n",
        "        start = i * batch_size\n",
        "        end = min(start + batch_size, n_samples)\n",
        "\n",
        "        batch = X[start:end]\n",
        "        batch_labels = labels[start:end]\n",
        "\n",
        "        unique_labels = np.unique(batch_labels)\n",
        "\n",
        "        for label in unique_labels:\n",
        "            cluster = batch[batch_labels == label]\n",
        "            intra_cluster_distance = np.max(pdist(cluster))\n",
        "\n",
        "            if intra_cluster_distance > max_intra_cluster_distance:\n",
        "                max_intra_cluster_distance = intra_cluster_distance\n",
        "\n",
        "        for i, c1 in enumerate(unique_labels):\n",
        "            for j, c2 in enumerate(unique_labels):\n",
        "                if i < j:\n",
        "                    inter_cluster_distance = np.min(cdist(X[labels == c1], X[labels == c2]))\n",
        "\n",
        "                    if inter_cluster_distance < min_inter_cluster_distance:\n",
        "                        min_inter_cluster_distance = inter_cluster_distance\n",
        "\n",
        "    return min_inter_cluster_distance / max_intra_cluster_distance\n",
        "\n",
        "\n",
        "# code to evaluate the clustering algorithms\n",
        "def evaluate_clustering(algorithm, dataset, true_labels=None, n_runs=5):\n",
        "    results = defaultdict(list)\n",
        "\n",
        "    metric_function_map = {\n",
        "        'cp': compactness,\n",
        "        'sp': separation,\n",
        "        'db': davies_bouldin_score,\n",
        "        'silhouette': silhouette_score,\n",
        "        'calinski_harabasz': calinski_harabasz_score,\n",
        "        'ari': adjusted_rand_score,\n",
        "        'nmi': normalized_mutual_info_score,\n",
        "        'dvi': dunn_index,\n",
        "    }\n",
        "\n",
        "    previous_runs_labels = []\n",
        "    previous_runs_results = []\n",
        "\n",
        "    # Start memory monitoring before the loop\n",
        "    tracemalloc.start()\n",
        "\n",
        "    for _ in range(n_runs):\n",
        "        with Timer() as t, DiskMonitor() as d:\n",
        "            labels = algorithm.fit_predict(dataset)\n",
        "\n",
        "        # Memory usage tracking\n",
        "        current_memory, peak_memory = tracemalloc.get_traced_memory()\n",
        "        results['memory_usage'].append(current_memory)\n",
        "        results['memory_peak'].append(peak_memory)\n",
        "\n",
        "        results['runtime'].append(t.interval)\n",
        "        results['disk_read'].append(d.disk_read_bytes)\n",
        "        results['disk_write'].append(d.disk_write_bytes)\n",
        "\n",
        "        if true_labels is not None:\n",
        "            previous_runs_labels.append(labels)\n",
        "\n",
        "            metric_results = {}\n",
        "\n",
        "            # Calculate clustering performance metrics\n",
        "            for metric_name, metric_function in metric_function_map.items():\n",
        "                try:\n",
        "                    value = metric_function(dataset, labels) if metric_name in {'calinski_harabasz', 'davies_bouldin_score', 'silhouette_score', 'cp', 'sp', 'dvi'} else metric_function(true_labels.flatten(), labels.flatten()) if metric_name in {'ari', 'nmi'} else metric_function(dataset, labels.reshape(-1, 1))\n",
        "                    metric_results[metric_name] = value\n",
        "                except ValueError as e:\n",
        "                    print(f\"Error calculating {metric_name}: {e}\")\n",
        "                    metric_results[metric_name] = np.nan\n",
        "\n",
        "            # Add the current run's results to the list of previous runs\n",
        "            previous_runs_results.append(metric_results)\n",
        "\n",
        "    # Stop memory monitoring after the loop\n",
        "    tracemalloc.stop()\n",
        "\n",
        "    # Aggregate the results over all runs\n",
        "    for metric_name in metric_function_map.keys():\n",
        "        metric_values = [run_result[metric_name] for run_result in previous_runs_results]\n",
        "        results[f\"{metric_name}_mean\"].append(np.mean(metric_values))\n",
        "        results[f\"{metric_name}_std\"].append(np.std(metric_values))\n",
        "        results[f\"{metric_name}_max\"].append(np.max(metric_values))\n",
        "        results[f\"{metric_name}_min\"].append(np.min(metric_values))\n",
        "\n",
        "    # Calculate min, max, mean, and std of memory usage over all runs\n",
        "    memory_usages = np.array(results['memory_usage'])\n",
        "    memory_stats = {\n",
        "        'memory_min': np.min(memory_usages),\n",
        "        'memory_max': np.max(memory_usages),\n",
        "        'memory_mean': np.mean(memory_usages),\n",
        "        'memory_std': np.std(memory_usages)\n",
        "    }\n",
        "\n",
        "    # Save memory stats\n",
        "    for stat_name, stat_value in memory_stats.items():\n",
        "        results[stat_name].append(stat_value)\n",
        "    results['runtime_mean'] = np.mean(results['runtime'])\n",
        "    results['runtime_std'] = np.std(results['runtime'])\n",
        "    results['runtime_max'] = np.max(results['runtime'])\n",
        "    results['runtime_min'] = np.min(results['runtime'])\n",
        "\n",
        "    return dict(results)"
      ],
      "metadata": {
        "id": "R4giXHuXWteg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5) Run all models against all data to evaluate metrics**"
      ],
      "metadata": {
        "id": "hWMQC0GXWeN-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "from google.oauth2 import service_account\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.errors import HttpError\n",
        "from googleapiclient.http import MediaFileUpload\n",
        "from sklearn import metrics\n",
        "import gc\n",
        "\n",
        "from google.colab import drive\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/gdrive', force_remount=False)\n",
        "\n",
        "# Set the path to the 'Masters_data' folder in your Google Drive\n",
        "base_path = '/content/gdrive/MyDrive/'\n",
        "folder_name = 'Masters_data'\n",
        "folder_path = os.path.join(base_path, folder_name)\n",
        "n_clusters = 3\n",
        "batch_size=10\n",
        "sampling_rate=0.9\n",
        "\\\n",
        "\n",
        "\n",
        "# Find all CSV files in the folder\n",
        "csv_files = glob.glob(os.path.join(folder_path, '*.csv'))\n",
        "print(csv_files)\n",
        "class NumPyEncoder(json.JSONEncoder):\n",
        "    def default(self, obj):\n",
        "        if isinstance(obj, np.integer):\n",
        "            return int(obj)\n",
        "        elif isinstance(obj, np.floating):\n",
        "            return float(obj)\n",
        "        elif isinstance(obj, np.ndarray):\n",
        "            return obj.tolist()\n",
        "        else:\n",
        "            return super(NumPyEncoder, self).default(obj)\n",
        "\n",
        "# Load the metadata\n",
        "metadata_file_path = os.path.join(folder_path, 'metadata.json')\n",
        "with open(metadata_file_path, 'r') as f:\n",
        "    metadata_list = json.load(f)\n",
        "\n",
        "# Convert the list to a dictionary for easier access\n",
        "metadata = {item['id']: item for item in metadata_list}\n",
        "\n",
        "\n",
        "\n",
        "# Loop through all CSV files\n",
        "for csv_file in csv_files:\n",
        "\n",
        "    # Use the CSV name as the ID of the dataset\n",
        "    dataset_id = os.path.splitext(os.path.basename(csv_file))[0]\n",
        "\n",
        "    # Extract the number from the dataset_id\n",
        "    dataset_number = int(dataset_id.split('_')[-1])\n",
        "\n",
        "    # Extract the n_clusters value for the current dataset from metadata\n",
        "    n_clusters = metadata[dataset_number]['n_clusters']\n",
        "\n",
        "    # Define the models\n",
        "    models = {\n",
        "    'KMeans': {\n",
        "        'id': 1,\n",
        "        'function': KMeans(n_clusters=n_clusters)\n",
        "    },\n",
        "    'S5C': {\n",
        "        'id': 2,\n",
        "        'function': S5C(n_clusters=n_clusters, n_nonzero_coefs=1000, T=1000, B=500)\n",
        "    },\n",
        "    'Minibatch_clustering': {\n",
        "        'id': 3,\n",
        "        'function': MiniBatchKMeans(n_clusters=n_clusters, batch_size=batch_size, random_state=42)\n",
        "    },\n",
        "    'Parallel K-means': {\n",
        "        'id': 4,\n",
        "        'function': MapReduceKMeans(n_clusters=n_clusters, batch_size=10, max_iterations=100)\n",
        "    },\n",
        "    'Weighted Consensus Clustering': {\n",
        "        'id': 5,\n",
        "        'function': ConsensusClustering(n_clusters, model_funcs, weights, sampling_rate)\n",
        "    }\n",
        "    }\n",
        "    # Load the dataset without headers\n",
        "    df = pd.read_csv(csv_file, header=None)\n",
        "\n",
        "    # Extract features and true labels\n",
        "    X = df.iloc[:, :-1].values  # all columns except the last\n",
        "    y_true = df.iloc[:, -1].values  # last column\n",
        "\n",
        "    # Loop through each model\n",
        "    for model_name, model_info in models.items():\n",
        "        print(model_name)\n",
        "        print(csv_file)\n",
        "        model = model_info['function']\n",
        "        # model.set_params(n_clusters=n_clusters)\n",
        "\n",
        "        # Evaluate the model on the dataset\n",
        "        results = evaluate_clustering(algorithm=model, dataset=X, true_labels=y_true, n_runs=1)\n",
        "\n",
        "        # Add dataset and model information to the results\n",
        "        results['dataset_id'] = dataset_id\n",
        "        results['model_id'] = model_info['id']\n",
        "        results['model'] = model_name\n",
        "        # Write the results to a new file\n",
        "        results_file_path = os.path.join(folder_path, f'results_eval.json')\n",
        "        with open(results_file_path, 'a') as f:\n",
        "            # And then when dumping the JSON:\n",
        "            json.dump(results, f, cls=NumPyEncoder)\n",
        "\n",
        "        results_file_path = os.path.join(folder_path, 'final_results_eval.json')\n",
        "\n",
        "        # Load the existing results, if any\n",
        "        if os.path.exists(results_file_path):\n",
        "            with open(results_file_path, 'r') as f:\n",
        "                existing_results = json.load(f)\n",
        "        else:\n",
        "            existing_results = []\n",
        "\n",
        "        # Append the new result\n",
        "        existing_results.append(results)\n",
        "\n",
        "        # Write the updated results back to the file\n",
        "        with open(results_file_path, 'w') as f:\n",
        "            json.dump(existing_results, f, cls=NumPyEncoder)\n",
        "\n",
        "        # Append the results to the final_evaluation_results file\n",
        "        # final_results_file_path = os.path.join(folder_path, 'final_evaluation_results.json')\n",
        "        # if os.path.exists(final_results_file_path):\n",
        "        #     with open(final_results_file_path, 'r') as f:\n",
        "        #         final_results = json.load(f)\n",
        "        # else:\n",
        "        #     final_results = []\n",
        "\n",
        "        # final_results.append(results)\n",
        "\n",
        "        # with open(final_results_file_path, 'w') as f:\n",
        "        #     json.dump(final_results, f,cls=NumPyEncoder)\n",
        "    gc.collect()"
      ],
      "metadata": {
        "id": "p9xG-sskWiiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Statistical analysis**"
      ],
      "metadata": {
        "id": "yjhGNAgCYrWs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.1) Friedman test"
      ],
      "metadata": {
        "id": "dOJ0flXtYyJF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rhGdmAOfY4_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.2) Nemenyi Test"
      ],
      "metadata": {
        "id": "iiXB-9ZTY5Tp"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Wu1wm6IPY_VX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}