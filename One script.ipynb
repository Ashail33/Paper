{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNKADpxFxz5tY36UHU8ba8B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ashail33/Paper/blob/master/One%20script.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1) Environment prep**"
      ],
      "metadata": {
        "id": "ImzJDJofLgxV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Set up for data generation\n",
        "!pip install git+https://github.com/Ashail33/mdcgenpy.git\n",
        "\n",
        "from  mdcgenpy import clusters as cl\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "import glob\n",
        "import re\n",
        "from google.oauth2 import service_account\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.errors import HttpError\n",
        "from googleapiclient.http import MediaFileUpload\n",
        "# import cl\n",
        "from google.colab import drive\n",
        "\n",
        "# Set up for model creation and evaluation\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.2.0/spark-3.2.0-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.0-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "import os\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.0-bin-hadoop3.2\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"my-app-name\").master(\"local[*]\").getOrCreate()\n",
        "\n",
        "\n",
        "\n",
        "from pyspark.ml.clustering import KMeans, KMeansModel\n",
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.sql import SparkSession\n",
        "from sklearn.base import BaseEstimator, ClusterMixin\n",
        "import numpy as np\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml.clustering import KMeans as PKMeans\n",
        "from sklearn.cluster import KMeans as SKKMeans\n",
        "from pyspark.ml.evaluation import ClusteringEvaluator\n",
        "import numpy as np\n",
        "\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.metrics import davies_bouldin_score, silhouette_score, calinski_harabasz_score, adjusted_rand_score, normalized_mutual_info_score\n",
        "from scipy.spatial.distance import cdist\n",
        "import time\n",
        "import psutil\n",
        "import os\n",
        "import tracemalloc\n",
        "from google.colab import drive\n",
        "import glob\n",
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans, DBSCAN, SpectralClustering\n",
        "from sklearn.mixture import GaussianMixture\n",
        "import json\n",
        "\n",
        "from sklearn.neighbors import KDTree\n",
        "import numpy as np\n",
        "from sklearn.metrics import pairwise_distances"
      ],
      "metadata": {
        "id": "tIGNr_dhLhW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2) Create Clustering Dataset**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-BuB0D1XLSIH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The properties I will be adding to the dataset are:\n",
        "#     1) Outliers - Binary ( two options) - outliers\n",
        "#     2) Noise - Binary ( two options) add_noise=0,n_noise=None,\n",
        "#     3) Number of clusters - I will select three values k\n",
        "#     4) Number of data points - I will select three values ( 100 000, 1 000 000, 100 000 000)n_samples\n",
        "#     5) Number of features - I will select five values ( 2, 10, 50, 100, 500) n_feats\n",
        "#     6) Density - I will select three values compactness_factor\n",
        "#     --7) Cluster shape - I will select three types - clusters within a radius ,  hollow shaped clusters , s-shaped / c-shaped clusters\n",
        "#      distributions\n",
        "#         'uniform': lambda shape, param: np.random.uniform(-param, param, shape),\n",
        "#     'gaussian': lambda shape, param: np.random.normal(0, param, shape),\n",
        "#     'logistic': lambda shape, param: np.random.logistic(0, param, shape),\n",
        "#     'triangular': lambda shape, param: np.random.triangular(-param, 0, param, shape),\n",
        "#     'gamma': lambda shape, param: np.random.gamma(2 + 8 * np.random.rand(), param / 5, shape),\n",
        "#     'gap': lambda shape, param: gap(shape, param)\n",
        "\n",
        "# --8) Missing values - this will need to be created by randomly removing values up to a certain number of columns and records\n",
        "\n",
        "#Mount drive where you would like to add the data to\n",
        "\n",
        "drive.mount('/content/gdrive', force_remount=False)\n",
        "\n",
        "# Set the path to the 'Masters_data' folder in your Google Drive\n",
        "base_path = '/content/gdrive/MyDrive/'\n",
        "folder_name = 'Masters_data'\n",
        "folder_path = os.path.join(base_path, folder_name)\n",
        "\n",
        "# Create the 'Masters_data' folder if it doesn't exist\n",
        "if not os.path.exists(folder_path):\n",
        "    os.makedirs(folder_path)\n",
        "\n",
        "# Parameters for generating datasets\n",
        "outliers = [0,500]\n",
        "noise_options = [(0, None), (10, 100)]\n",
        "n_clusters = [3, 5, 10]\n",
        "n_samples = [10000,50000, 100000]\n",
        "n_feats = [2, 10,100]\n",
        "compactness_factors = [0.01, 0.5, 1]\n",
        "\n",
        "# Distributions for cluster shapes\n",
        "dist_options = ['gaussian']\n",
        "\n",
        "# Find the last generated dataset\n",
        "existing_files = glob.glob(os.path.join(folder_path, 'dataset_*.csv'))\n",
        "dataset_numbers = [int(re.search(r'dataset_(\\d+).csv', file).group(1)) for file in existing_files]\n",
        "if dataset_numbers:\n",
        "    last_dataset_id = max(dataset_numbers)\n",
        "else:\n",
        "    last_dataset_id = -1\n",
        "\n",
        "metadata = []\n",
        "\n",
        "# Continue from the last generated dataset\n",
        "for idx, outlier in enumerate(outliers):\n",
        "    for idy, (add_noise, n_noise) in enumerate(noise_options):\n",
        "        for idz, k in enumerate(n_clusters):\n",
        "            for idw, n_sample in enumerate(n_samples):\n",
        "                for idv, n_feat in enumerate(n_feats):\n",
        "                    for idu, compactness_factor in enumerate(compactness_factors):\n",
        "                        for idt, distribution in enumerate(dist_options):\n",
        "                            current_dataset_id = (idx * len(noise_options) * len(n_clusters) * len(n_samples) * len(n_feats) * len(compactness_factors) * len(dist_options) +\n",
        "                                                  idy * len(n_clusters) * len(n_samples) * len(n_feats) * len(compactness_factors) * len(dist_options) +\n",
        "                                                  idz * len(n_samples) * len(n_feats) * len(compactness_factors) * len(dist_options) +\n",
        "                                                  idw * len(n_feats) * len(compactness_factors) * len(dist_options) +\n",
        "                                                  idv * len(compactness_factors) * len(dist_options) +\n",
        "                                                  idu * len(dist_options) +\n",
        "                                                  idt)\n",
        "\n",
        "                            if current_dataset_id <= last_dataset_id:\n",
        "                                continue\n",
        "\n",
        "                            file_name = f'dataset_{current_dataset_id}.csv'\n",
        "                            file_path = os.path.join(folder_path, file_name)\n",
        "                            distributions = [distribution] * k\n",
        "\n",
        "\n",
        "                            cluster_gen = cl.ClusterGenerator(\n",
        "                                          n_samples=n_sample,\n",
        "                                          outliers=outlier,\n",
        "                                          n_feats=n_feat,\n",
        "                                          k=k,\n",
        "                                          distributions=distributions,\n",
        "                                          compactness_factor=compactness_factor,\n",
        "\n",
        "                                          )\n",
        "\n",
        "                            data = cluster_gen.generate_data(output_file=file_path)\n",
        "\n",
        "                            dataset_properties = {\n",
        "                                'id': current_dataset_id,\n",
        "                                'outliers': outlier,\n",
        "                                'add_noise': add_noise,\n",
        "                                'n_noise': n_noise,'n_clusters': k,\n",
        "                                'n_samples': n_sample,\n",
        "                                'n_feats': n_feat,\n",
        "                                'compactness_factor': compactness_factor,\n",
        "                                'distribution': distribution,\n",
        "                                'file_path': file_path\n",
        "                                }\n",
        "\n",
        "                            metadata.append(dataset_properties)\n",
        "                            metadata_file_path = os.path.join(folder_path, 'metadata.json')\n",
        "                            with open(metadata_file_path, 'w') as f:\n",
        "                                json.dump(metadata, f)\n"
      ],
      "metadata": {
        "id": "dvz7jsKwLe3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3) Create the model functions**"
      ],
      "metadata": {
        "id": "N0yx-q0VPFvg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1) Standard SK-learn models\n",
        "\n"
      ],
      "metadata": {
        "id": "iYeBvFV-Xn0m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import MiniBatchKMeans\n"
      ],
      "metadata": {
        "id": "WdbsvszsPMqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2)"
      ],
      "metadata": {
        "id": "9jTzmFyqX16a"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Nqv2VyFYYYD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4) Create the Evaluation function**"
      ],
      "metadata": {
        "id": "Gem7BXXjWv2k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "#evaluation code\n",
        "class Timer:\n",
        "    def __enter__(self):\n",
        "        self.start_time = time.monotonic()\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "        self.interval = time.monotonic() - self.start_time\n",
        "\n",
        "\n",
        "class MemoryMonitor:\n",
        "    def __enter__(self):\n",
        "        self.process = psutil.Process(os.getpid())\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "        self.memory_info = self.process.memory_info()\n",
        "\n",
        "\n",
        "class DiskMonitor:\n",
        "    def __enter__(self):\n",
        "        self.disk_read_start = psutil.disk_io_counters().read_bytes\n",
        "        self.disk_write_start = psutil.disk_io_counters().write_bytes\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "        self.disk_read_end = psutil.disk_io_counters().read_bytes\n",
        "        self.disk_write_end = psutil.disk_io_counters().write_bytes\n",
        "        self.disk_read_bytes = self.disk_read_end - self.disk_read_start\n",
        "        self.disk_write_bytes = self.disk_write_end - self.disk_write_start\n",
        "\n",
        "\n",
        "def compactness(X, labels):\n",
        "    labels = labels.flatten()\n",
        "    unique_labels = np.unique(labels)\n",
        "    centroids = np.array([np.mean(X[labels == i], axis=0) for i in unique_labels])\n",
        "    distances = cdist(X, centroids)\n",
        "    return np.sum(np.min(distances, axis=1))\n",
        "\n",
        "\n",
        "def separation(X, labels):\n",
        "    labels = labels.flatten()\n",
        "    unique_labels = np.unique(labels)\n",
        "    centroids = np.array([np.mean(X[labels == i], axis=0) for i in unique_labels])\n",
        "    return np.sum(cdist(centroids, centroids))\n",
        "\n",
        "\n",
        "\n",
        "def batched_silhouette_score(X, labels, batch_size=100000):\n",
        "    labels = labels.flatten()\n",
        "    n_samples = len(X)\n",
        "    n_batches = int(np.ceil(n_samples / batch_size))\n",
        "\n",
        "    silhouette_sum = 0\n",
        "\n",
        "    for i in range(n_batches):\n",
        "        start = i * batch_size\n",
        "        end = min(start + batch_size, n_samples)\n",
        "\n",
        "        pairwise_distances = metrics.pairwise_distances(X[start:end], X)\n",
        "\n",
        "        a = np.array([np.mean(pairwise_distances[i, labels == labels[start + i]]) for i in range(end-start)])\n",
        "        b = np.array([np.min([np.mean(pairwise_distances[i, labels == cur_label])\n",
        "                              for cur_label in set(labels) - {labels[start + i]}])\n",
        "                      for i in range(end-start)])\n",
        "\n",
        "        silhouette_sum += np.sum((b - a) / np.maximum(a, b))\n",
        "\n",
        "    return silhouette_sum / n_samples\n",
        "\n",
        "def dunn_index(X, labels):\n",
        "    labels = labels.flatten()\n",
        "    unique_labels = np.unique(labels)\n",
        "    clusters = [X[labels == i] for i in unique_labels]\n",
        "    max_intra_cluster_distance = np.max([np.max(cdist(c1, c1)) for c1 in clusters])\n",
        "\n",
        "    min_inter_cluster_distance = np.inf\n",
        "    for i, c1 in enumerate(clusters):\n",
        "        for j, c2 in enumerate(clusters):\n",
        "            if i < j:\n",
        "                min_distance = np.min(cdist(c1, c2))\n",
        "                if min_distance < min_inter_cluster_distance:\n",
        "                    min_inter_cluster_distance = min_distance\n",
        "\n",
        "    return min_inter_cluster_distance / max_intra_cluster_distance\n",
        "\n",
        "from scipy.spatial.distance import pdist, cdist\n",
        "\n",
        "def batched_dunn_index(X, labels, batch_size=100000):\n",
        "    n_samples = len(X)\n",
        "    n_batches = int(np.ceil(n_samples / batch_size))\n",
        "\n",
        "    max_intra_cluster_distance = -np.inf\n",
        "    min_inter_cluster_distance = np.inf\n",
        "\n",
        "    for i in range(n_batches):\n",
        "        start = i * batch_size\n",
        "        end = min(start + batch_size, n_samples)\n",
        "\n",
        "        batch = X[start:end]\n",
        "        batch_labels = labels[start:end]\n",
        "\n",
        "        unique_labels = np.unique(batch_labels)\n",
        "\n",
        "        for label in unique_labels:\n",
        "            cluster = batch[batch_labels == label]\n",
        "            intra_cluster_distance = np.max(pdist(cluster))\n",
        "\n",
        "            if intra_cluster_distance > max_intra_cluster_distance:\n",
        "                max_intra_cluster_distance = intra_cluster_distance\n",
        "\n",
        "        for i, c1 in enumerate(unique_labels):\n",
        "            for j, c2 in enumerate(unique_labels):\n",
        "                if i < j:\n",
        "                    inter_cluster_distance = np.min(cdist(X[labels == c1], X[labels == c2]))\n",
        "\n",
        "                    if inter_cluster_distance < min_inter_cluster_distance:\n",
        "                        min_inter_cluster_distance = inter_cluster_distance\n",
        "\n",
        "    return min_inter_cluster_distance / max_intra_cluster_distance\n",
        "\n",
        "\n",
        "# code to evaluate the clustering algorithms\n",
        "def evaluate_clustering(algorithm, dataset, true_labels=None, n_runs=5):\n",
        "    results = defaultdict(list)\n",
        "\n",
        "    metric_function_map = {\n",
        "        'cp': compactness,\n",
        "        'sp': separation,\n",
        "        'db': davies_bouldin_score,\n",
        "        'silhouette': silhouette_score,\n",
        "        'calinski_harabasz': calinski_harabasz_score,\n",
        "        'ari': adjusted_rand_score,\n",
        "        'nmi': normalized_mutual_info_score,\n",
        "        'dvi': dunn_index,\n",
        "    }\n",
        "\n",
        "    previous_runs_labels = []\n",
        "    previous_runs_results = []\n",
        "\n",
        "    # Start memory monitoring before the loop\n",
        "    tracemalloc.start()\n",
        "\n",
        "    for _ in range(n_runs):\n",
        "        with Timer() as t, DiskMonitor() as d:\n",
        "            labels = algorithm.fit_predict(dataset)\n",
        "\n",
        "        # Memory usage tracking\n",
        "        current_memory, peak_memory = tracemalloc.get_traced_memory()\n",
        "        results['memory_usage'].append(current_memory)\n",
        "        results['memory_peak'].append(peak_memory)\n",
        "\n",
        "        results['runtime'].append(t.interval)\n",
        "        results['disk_read'].append(d.disk_read_bytes)\n",
        "        results['disk_write'].append(d.disk_write_bytes)\n",
        "\n",
        "        if true_labels is not None:\n",
        "            previous_runs_labels.append(labels)\n",
        "\n",
        "            metric_results = {}\n",
        "\n",
        "            # Calculate clustering performance metrics\n",
        "            for metric_name, metric_function in metric_function_map.items():\n",
        "                try:\n",
        "                    value = metric_function(dataset, labels) if metric_name in {'calinski_harabasz', 'davies_bouldin_score', 'silhouette_score', 'cp', 'sp', 'dvi'} else metric_function(true_labels.flatten(), labels.flatten()) if metric_name in {'ari', 'nmi'} else metric_function(dataset, labels.reshape(-1, 1))\n",
        "                    metric_results[metric_name] = value\n",
        "                except ValueError as e:\n",
        "                    print(f\"Error calculating {metric_name}: {e}\")\n",
        "                    metric_results[metric_name] = np.nan\n",
        "\n",
        "            # Add the current run's results to the list of previous runs\n",
        "            previous_runs_results.append(metric_results)\n",
        "\n",
        "    # Stop memory monitoring after the loop\n",
        "    tracemalloc.stop()\n",
        "\n",
        "    # Aggregate the results over all runs\n",
        "    for metric_name in metric_function_map.keys():\n",
        "        metric_values = [run_result[metric_name] for run_result in previous_runs_results]\n",
        "        results[f\"{metric_name}_mean\"].append(np.mean(metric_values))\n",
        "        results[f\"{metric_name}_std\"].append(np.std(metric_values))\n",
        "        results[f\"{metric_name}_max\"].append(np.max(metric_values))\n",
        "        results[f\"{metric_name}_min\"].append(np.min(metric_values))\n",
        "\n",
        "    # Calculate min, max, mean, and std of memory usage over all runs\n",
        "    memory_usages = np.array(results['memory_usage'])\n",
        "    memory_stats = {\n",
        "        'memory_min': np.min(memory_usages),\n",
        "        'memory_max': np.max(memory_usages),\n",
        "        'memory_mean': np.mean(memory_usages),\n",
        "        'memory_std': np.std(memory_usages)\n",
        "    }\n",
        "\n",
        "    # Save memory stats\n",
        "    for stat_name, stat_value in memory_stats.items():\n",
        "        results[stat_name].append(stat_value)\n",
        "    results['runtime_mean'] = np.mean(results['runtime'])\n",
        "    results['runtime_std'] = np.std(results['runtime'])\n",
        "    results['runtime_max'] = np.max(results['runtime'])\n",
        "    results['runtime_min'] = np.min(results['runtime'])\n",
        "\n",
        "    return dict(results)"
      ],
      "metadata": {
        "id": "R4giXHuXWteg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5) Run all models against all data to evaluate metrics**"
      ],
      "metadata": {
        "id": "hWMQC0GXWeN-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "from google.oauth2 import service_account\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.errors import HttpError\n",
        "from googleapiclient.http import MediaFileUpload\n",
        "from sklearn import metrics\n",
        "import gc\n",
        "\n",
        "from google.colab import drive\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/gdrive', force_remount=False)\n",
        "\n",
        "# Set the path to the 'Masters_data' folder in your Google Drive\n",
        "base_path = '/content/gdrive/MyDrive/'\n",
        "folder_name = 'Masters_data'\n",
        "folder_path = os.path.join(base_path, folder_name)\n",
        "n_clusters = 3\n",
        "batch_size=10\n",
        "sampling_rate=0.9\n",
        "\\\n",
        "\n",
        "\n",
        "# Find all CSV files in the folder\n",
        "csv_files = glob.glob(os.path.join(folder_path, '*.csv'))\n",
        "print(csv_files)\n",
        "class NumPyEncoder(json.JSONEncoder):\n",
        "    def default(self, obj):\n",
        "        if isinstance(obj, np.integer):\n",
        "            return int(obj)\n",
        "        elif isinstance(obj, np.floating):\n",
        "            return float(obj)\n",
        "        elif isinstance(obj, np.ndarray):\n",
        "            return obj.tolist()\n",
        "        else:\n",
        "            return super(NumPyEncoder, self).default(obj)\n",
        "\n",
        "# Load the metadata\n",
        "metadata_file_path = os.path.join(folder_path, 'metadata.json')\n",
        "with open(metadata_file_path, 'r') as f:\n",
        "    metadata_list = json.load(f)\n",
        "\n",
        "# Convert the list to a dictionary for easier access\n",
        "metadata = {item['id']: item for item in metadata_list}\n",
        "\n",
        "\n",
        "\n",
        "# Loop through all CSV files\n",
        "for csv_file in csv_files:\n",
        "\n",
        "    # Use the CSV name as the ID of the dataset\n",
        "    dataset_id = os.path.splitext(os.path.basename(csv_file))[0]\n",
        "\n",
        "    # Extract the number from the dataset_id\n",
        "    dataset_number = int(dataset_id.split('_')[-1])\n",
        "\n",
        "    # Extract the n_clusters value for the current dataset from metadata\n",
        "    n_clusters = metadata[dataset_number]['n_clusters']\n",
        "\n",
        "    # Define the models\n",
        "    models = {\n",
        "    'KMeans': {\n",
        "        'id': 1,\n",
        "        'function': KMeans(n_clusters=n_clusters)\n",
        "    },\n",
        "    'S5C': {\n",
        "        'id': 2,\n",
        "        'function': S5C(n_clusters=n_clusters, n_nonzero_coefs=1000, T=1000, B=500)\n",
        "    },\n",
        "    'Minibatch_clustering': {\n",
        "        'id': 3,\n",
        "        'function': MiniBatchKMeans(n_clusters=n_clusters, batch_size=batch_size, random_state=42)\n",
        "    },\n",
        "    'Parallel K-means': {\n",
        "        'id': 4,\n",
        "        'function': MapReduceKMeans(n_clusters=n_clusters, batch_size=10, max_iterations=100)\n",
        "    },\n",
        "    'Weighted Consensus Clustering': {\n",
        "        'id': 5,\n",
        "        'function': ConsensusClustering(n_clusters, model_funcs, weights, sampling_rate)\n",
        "    }\n",
        "    }\n",
        "    # Load the dataset without headers\n",
        "    df = pd.read_csv(csv_file, header=None)\n",
        "\n",
        "    # Extract features and true labels\n",
        "    X = df.iloc[:, :-1].values  # all columns except the last\n",
        "    y_true = df.iloc[:, -1].values  # last column\n",
        "\n",
        "    # Loop through each model\n",
        "    for model_name, model_info in models.items():\n",
        "        print(model_name)\n",
        "        print(csv_file)\n",
        "        model = model_info['function']\n",
        "        # model.set_params(n_clusters=n_clusters)\n",
        "\n",
        "        # Evaluate the model on the dataset\n",
        "        results = evaluate_clustering(algorithm=model, dataset=X, true_labels=y_true, n_runs=1)\n",
        "\n",
        "        # Add dataset and model information to the results\n",
        "        results['dataset_id'] = dataset_id\n",
        "        results['model_id'] = model_info['id']\n",
        "        results['model'] = model_name\n",
        "        # Write the results to a new file\n",
        "        results_file_path = os.path.join(folder_path, f'results_eval.json')\n",
        "        with open(results_file_path, 'a') as f:\n",
        "            # And then when dumping the JSON:\n",
        "            json.dump(results, f, cls=NumPyEncoder)\n",
        "\n",
        "        results_file_path = os.path.join(folder_path, 'final_results_eval.json')\n",
        "\n",
        "        # Load the existing results, if any\n",
        "        if os.path.exists(results_file_path):\n",
        "            with open(results_file_path, 'r') as f:\n",
        "                existing_results = json.load(f)\n",
        "        else:\n",
        "            existing_results = []\n",
        "\n",
        "        # Append the new result\n",
        "        existing_results.append(results)\n",
        "\n",
        "        # Write the updated results back to the file\n",
        "        with open(results_file_path, 'w') as f:\n",
        "            json.dump(existing_results, f, cls=NumPyEncoder)\n",
        "\n",
        "        # Append the results to the final_evaluation_results file\n",
        "        # final_results_file_path = os.path.join(folder_path, 'final_evaluation_results.json')\n",
        "        # if os.path.exists(final_results_file_path):\n",
        "        #     with open(final_results_file_path, 'r') as f:\n",
        "        #         final_results = json.load(f)\n",
        "        # else:\n",
        "        #     final_results = []\n",
        "\n",
        "        # final_results.append(results)\n",
        "\n",
        "        # with open(final_results_file_path, 'w') as f:\n",
        "        #     json.dump(final_results, f,cls=NumPyEncoder)\n",
        "    gc.collect()"
      ],
      "metadata": {
        "id": "p9xG-sskWiiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Statistical analysis**"
      ],
      "metadata": {
        "id": "yjhGNAgCYrWs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.1) Friedman test"
      ],
      "metadata": {
        "id": "dOJ0flXtYyJF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rhGdmAOfY4_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.2) Nemenyi Test"
      ],
      "metadata": {
        "id": "iiXB-9ZTY5Tp"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Wu1wm6IPY_VX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}